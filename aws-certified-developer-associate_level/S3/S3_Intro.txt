S3(Simple secure storage):
=================================

- One aws account can have only 100 buckets.
- bucket name should comply with DNS naming conventions.
- Bucket name must be unique, and follow the standards and must not be ip address.
- No uppercase or underscores.
- lowercase, numbers, ifens, periods are allowed.
- Periods(.)and Ifens(-) do not follow each other in the bucket name.
- S3 regions should be near to customers to optimize latency, minimize costs.
- Only 100 buckets can be created at a time in aws account.
- bucket ownership cannot be transferred to another account once a bucket is created.

Objects:
=============
In cloud, all folder and files under the buckets are referred as objects.
Maximum size of the objects can be 5 TB.
Objects larger than 5 gb require multipart upload API.
  - break the object and upload independently and in parallel.
  - if any part failed, we are retransmit only the failed part.
  - object can be reassembled after calling CompleteMultiPartUpload API.
Multipart Upload is recommended for uploading file size greater than 100MB.
Object can be encrypted and decrypted while downloading.
Each objects must be assigned with storage class, which determines the object availability, durability and cost.
By default all objects are private.
Automatically switch to different storage class or deleted(via lifecyle policies)


Data consistency:
=======================
- Read-after-write(we can read only after it's successfully written) consistency for PUTS of new object.
- Eventual consistency for overwrite PUTS and DELETE.

S3 Performance:
=================

- s3 maintains the index of object key names in each aws region.
     - object keys are stored across multiple partitions in that index.
     - key name is used to decide which partition the key is stored in.
     - similar object keys are stored in the same partitions. So, to avoid skewness in one partition, prefix some random number the object name.

- get request intensive workloads.
     - use cloudfront.
     - distributes content near to user which gives low latency and high transfer rate.
     - catches object
     - fewer direct requests to s3.

- scale to very high request rates.
- Burst in the number of request per second
    - >=300 put/list/delete
    - >=800 get
    - contact amazon to avoid temporary limits.
- need to consider the consistent high number of request per second when
    - >=100 put/list/delete
    - >=300 get


S3 Error Handling:
======================

404 - not found
403 - forbidden
400 - bad request
409 - conflict
500 - internal server error
